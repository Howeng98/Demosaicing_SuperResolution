{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「UnderGraduate_Project.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WIhqrzAFMqT",
        "outputId": "46eed7dd-745a-4ffb-a365-cd92ed5b09c2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 30 16:36:24 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7nY_VATIj-s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1366e4-eb80-45ce-e084-25bf50162c2a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5PMYUZu3NP2"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf \n",
        "from keras.models import Model,load_model\n",
        "import os\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import Lambda\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import backend as K\n",
        "import random\n",
        "from PIL import Image \n",
        "from random import shuffle\n",
        "%load_ext tensorboard\n",
        "import datetime\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5PwooDorNkc"
      },
      "source": [
        "**Prepare**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfOiNyPJFcFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b1c6be-0260-45d1-a2a8-bf95005a0e48"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "from numpy import asarray\n",
        "import math\n",
        "import copy\n",
        "import os\n",
        "from PIL import Image \n",
        "import cv2\n",
        "import shutil\n",
        "\n",
        "patch_size = 64 #input = 64x64\n",
        "label_size = 128 #output = 128x128\n",
        "\n",
        "#get RGGB bayer image\n",
        "def bayer_reverse(img):\n",
        "    height,width,c = img.shape;\n",
        "    tmp = np.zeros([height,width]);\n",
        "    for i in range( height ):\n",
        "        for j in range( width ):\n",
        "            if i % 2 == 0 :\n",
        "                if j % 2 == 0:\n",
        "                    tmp[i][j] = img[i][j][0];#R\n",
        "                else:\n",
        "                    tmp[i][j] = img[i][j][1];#G\n",
        "            else :\n",
        "                if j % 2 == 0:\n",
        "                    tmp[i][j] = img[i][j][1];#G\n",
        "                else:\n",
        "                    tmp[i][j] = img[i][j][2];#B\n",
        "\n",
        "    return tmp;\n",
        "\n",
        "#split image to prepare the train set\n",
        "def split(img,name,dir_path):\n",
        "    height,width,c = img.shape;    \n",
        "    count = 0;\n",
        "\n",
        "    for i in range(0, height, 30):\n",
        "        for j in range(0, width, 30):\n",
        "            if( i + label_size < height and j + label_size < width ):                \n",
        "                label = np.zeros([label_size,label_size,3])                \n",
        "                tmp2  = np.zeros([label_size,label_size,3])                \n",
        "                tmp2 = img[ i : i + label_size, j : j + label_size,:]\n",
        "                label_img = Image.fromarray(tmp2)\n",
        "\n",
        "                label[:,:,0] = tmp2[:,:,2]\n",
        "                label[:,:,1] = tmp2[:,:,1]\n",
        "                label[:,:,2] = tmp2[:,:,0]\n",
        "                label_path = os.path.join(dir_path,'label/'+name.split('.')[0] +'_'+str(count)+'.png')                                                \n",
        "                cv2.imwrite(label_path,label)\n",
        "\n",
        "                \n",
        "                zoom = label_img.resize((patch_size,patch_size), Image.BICUBIC)                 \n",
        "                tmp3 = np.zeros([patch_size,patch_size])\n",
        "                patch = np.zeros([patch_size,patch_size])\n",
        "\n",
        "                zoom2 = np.array(zoom)                \n",
        "                patch = bayer_reverse(zoom2)                \n",
        "                                           \n",
        "                patch_path = os.path.join(dir_path,'patch/'+name.split('.')[0] +'_'+str(count)+'.png')                          \n",
        "                cv2.imwrite(patch_path, patch)\n",
        "\n",
        "                count = count + 1    \n",
        "\n",
        "def main():\n",
        "    path = 'drive/MyDrive/Colab Notebooks/undergraduate_project'\n",
        "\n",
        "    if os.path.exists(os.path.join(path,'patch')):\n",
        "      shutil.rmtree(os.path.join(path,'patch'))\n",
        "    os.makedirs(os.path.join(path,'patch'))\n",
        "\n",
        "    if os.path.exists(os.path.join(path,'label')):\n",
        "      shutil.rmtree(os.path.join(path,'label'))\n",
        "    os.makedirs(os.path.join(path,'label'))\n",
        "\n",
        "\n",
        "    dataset_path = os.path.join(path,'BSD200')\n",
        "    entries = os.listdir(dataset_path)\n",
        "    for entry in entries:\n",
        "        print(entry)  \n",
        "        img_path = dataset_path + '/' + entry\n",
        "        img = Image.open(img_path)\n",
        "        img = np.array(img)   \n",
        "        split(img,entry,path)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "254054.png\n",
            "183055.png\n",
            "56028.png\n",
            "117054.png\n",
            "245051.png\n",
            "236017.png\n",
            "376001.png\n",
            "42078.png\n",
            "368078.png\n",
            "16052.png\n",
            "15088.png\n",
            "181018.png\n",
            "159091.png\n",
            "181091.png\n",
            "97017.png\n",
            "54005.png\n",
            "311068.png\n",
            "76002.png\n",
            "105053.png\n",
            "145053.png\n",
            "153077.png\n",
            "254033.png\n",
            "106025.png\n",
            "105019.png\n",
            "376020.png\n",
            "130034.png\n",
            "41004.png\n",
            "15004.png\n",
            "274007.png\n",
            "35091.png\n",
            "80099.png\n",
            "65019.png\n",
            "113044.png\n",
            "100098.png\n",
            "202012.png\n",
            "35010.png\n",
            "112082.png\n",
            "20008.png\n",
            "169012.png\n",
            "198004.png\n",
            "249087.png\n",
            "23080.png\n",
            "108073.png\n",
            "100080.png\n",
            "46076.png\n",
            "170054.png\n",
            "109034.png\n",
            "92059.png\n",
            "161062.png\n",
            "246016.png\n",
            "239096.png\n",
            "122048.png\n",
            "187029.png\n",
            "104022.png\n",
            "27059.png\n",
            "271008.png\n",
            "138078.png\n",
            "147062.png\n",
            "239007.png\n",
            "326038.png\n",
            "227040.png\n",
            "157036.png\n",
            "55075.png\n",
            "71046.png\n",
            "310007.png\n",
            "216066.png\n",
            "216041.png\n",
            "23084.png\n",
            "8049.png\n",
            "260081.png\n",
            "299091.png\n",
            "24063.png\n",
            "61060.png\n",
            "188063.png\n",
            "94079.png\n",
            "35008.png\n",
            "268002.png\n",
            "108041.png\n",
            "365025.png\n",
            "134052.png\n",
            "232038.png\n",
            "95006.png\n",
            "385028.png\n",
            "147021.png\n",
            "181079.png\n",
            "55067.png\n",
            "365073.png\n",
            "246053.png\n",
            "242078.png\n",
            "151087.png\n",
            "43083.png\n",
            "156079.png\n",
            "35058.png\n",
            "189003.png\n",
            "12003.png\n",
            "22090.png\n",
            "178054.png\n",
            "189011.png\n",
            "45077.png\n",
            "65010.png\n",
            "66075.png\n",
            "164074.png\n",
            "323016.png\n",
            "42044.png\n",
            "59078.png\n",
            "35070.png\n",
            "317080.png\n",
            "145014.png\n",
            "293029.png\n",
            "2092.png\n",
            "314016.png\n",
            "135037.png\n",
            "247085.png\n",
            "8143.png\n",
            "231015.png\n",
            "361084.png\n",
            "271031.png\n",
            "183087.png\n",
            "176039.png\n",
            "209070.png\n",
            "126039.png\n",
            "22013.png\n",
            "118020.png\n",
            "198054.png\n",
            "207056.png\n",
            "196015.png\n",
            "66039.png\n",
            "311081.png\n",
            "159029.png\n",
            "368016.png\n",
            "78019.png\n",
            "24004.png\n",
            "113009.png\n",
            "23025.png\n",
            "26031.png\n",
            "374067.png\n",
            "12074.png\n",
            "113016.png\n",
            "155060.png\n",
            "253036.png\n",
            "370036.png\n",
            "238011.png\n",
            "33066.png\n",
            "65074.png\n",
            "61086.png\n",
            "225017.png\n",
            "60079.png\n",
            "41025.png\n",
            "103041.png\n",
            "301007.png\n",
            "286092.png\n",
            "28075.png\n",
            "388016.png\n",
            "106020.png\n",
            "188005.png\n",
            "309004.png\n",
            "277095.png\n",
            "22093.png\n",
            "216053.png\n",
            "176035.png\n",
            "302003.png\n",
            "166081.png\n",
            "68077.png\n",
            "118035.png\n",
            "292066.png\n",
            "153093.png\n",
            "188091.png\n",
            "187083.png\n",
            "144067.png\n",
            "353013.png\n",
            "134008.png\n",
            "43070.png\n",
            "163062.png\n",
            "48055.png\n",
            "140055.png\n",
            "285036.png\n",
            "25098.png\n",
            "140075.png\n",
            "187039.png\n",
            "187003.png\n",
            "172032.png\n",
            "90076.png\n",
            "28096.png\n",
            "372047.png\n",
            "65132.png\n",
            "67079.png\n",
            "374020.png\n",
            "187071.png\n",
            "173036.png\n",
            "100075.png\n",
            "159045.png\n",
            "138032.png\n",
            "198023.png\n",
            "227046.png\n",
            "249061.png\n",
            "87065.png\n",
            "176019.png\n",
            "124084.png\n",
            "135069.png\n",
            "163014.png\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geAXuo4nrFWD"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_SV9g9WWp9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e84fa597-9240-4b56-a1c7-cd6b3ae4f0df"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf \n",
        "from keras.models import Model,load_model\n",
        "import os\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
        "from keras.layers import Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "import random\n",
        "from PIL import Image \n",
        "from random import shuffle\n",
        "\n",
        "train_image = []\n",
        "train_label = []\n",
        "\n",
        "dir_path = 'drive/MyDrive/Colab Notebooks/undergraduate_project'\n",
        "patch_path = os.path.join(dir_path,'patch')\n",
        "entries = os.listdir(patch_path)\n",
        "for entry in entries:  \n",
        "  im = Image.open(patch_path+'/'+entry)\n",
        "  img = image.img_to_array(im)\n",
        "  # print(img.shape)\n",
        "  # img = img/255.\n",
        "  train_image.append(img)\n",
        "train_image= np.stack(train_image)\n",
        "\n",
        "print(train_image.shape)\n",
        "  \n",
        "\n",
        "label_path = os.path.join(dir_path,'label')\n",
        "entries = os.listdir(label_path)\n",
        "for entry in entries:  \n",
        "  im = Image.open(label_path+'/'+entry)\n",
        "  img = image.img_to_array(im)\n",
        "  # img = img/255.\n",
        "  train_label.append(img)\n",
        "train_label = np.stack(train_label)\n",
        "\n",
        "print(train_label.shape)\n",
        "\n",
        "\n",
        "\n",
        "index = [i for i in range(train_image.shape[0])]\n",
        "shuffle(index)\n",
        "train_image = train_image[index,:,:,:];\n",
        "train_label = train_label[index,:,:,:];\n",
        "\n",
        "# np.save('drive/My Drive/Colab Notebooks/undergraduate_project/train_image.npy', train_image)\n",
        "# np.save('drive/My Drive/Colab Notebooks/undergraduate_project/train_label.npy', train_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16800, 64, 64, 1)\n",
            "(16800, 128, 128, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ClU51qwkcz3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b7d986-3ca3-4724-ca6e-688390d4c287"
      },
      "source": [
        "############################# Model Structure ################################################\n",
        "def create_model():\n",
        "  inputs = keras.Input(shape=(None,None,1))\n",
        "\n",
        "  ini = keras.layers.Conv2D(filters = 128, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2                     \n",
        "                     padding = 'same', \n",
        "                     input_shape = (None,None,1))(inputs)\n",
        "\n",
        "  x = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(ini)\n",
        "  \n",
        "  ini = keras.layers.Conv2D(filters = 128, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     activation = 'relu',\n",
        "                     padding = 'same', \n",
        "                     input_shape = (None,None,128))(x)\n",
        "\n",
        "  x = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(ini)\n",
        "\n",
        "  ini = keras.layers.Conv2D(filters = 64, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     activation = 'relu',\n",
        "                     padding = 'same', \n",
        "                     input_shape = (None,None,128))(x)\n",
        "\n",
        "  x = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(ini)\n",
        "\n",
        "  ##Subpixel Construction\n",
        "  sub_layer_2 = Lambda(lambda x:tf.nn.space_to_depth(x,2)) \n",
        "  init = sub_layer_2(inputs=x)\n",
        "\n",
        "\n",
        "  ##Learning Residual (DCNN)\n",
        "  ####Conv 3x3x64x64 + PReLu\n",
        "  x = keras.layers.Conv2D(filters = 64, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same', \n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,1))(init)\n",
        "  x = keras.layers.BatchNormalization()(x)                     \n",
        "  x = keras.layers.Conv2D(filters = 64, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same', \n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.BatchNormalization()(x)                     \n",
        "  x = keras.layers.Conv2D(filters = 64, #feature map number\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same', \n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.BatchNormalization()(x)                     \n",
        "  # x = keras.layers.MaxPooling2D((2,2),padding='same')(x)\n",
        "  # x = keras.layers.BatchNormalization()(x)  \n",
        "  # x = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(x)\n",
        "\n",
        "  ####Residual Block\n",
        "  for i in range(6):\n",
        "    start = x\n",
        "    Conv1 = keras.layers.Conv2D(filters=64,\n",
        "                        kernel_size = 1, \n",
        "                        strides = 1,  # 2\n",
        "                        padding = 'same',                                             \n",
        "                        input_shape = (None,None,64))(start)\n",
        "    Conv1 = keras.layers.LeakyReLU()(Conv1)\n",
        "    # Conv1 = keras.layers.MaxPooling2D((2,2),padding='same')(Conv1)\n",
        "    # Conv1_BN = keras.layers.BatchNormalization()(Conv1)\n",
        "    # Conv1_BN = Dropout(0.5)(Conv1_BN)\n",
        "    \n",
        "    # PReLu = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(Conv1)\n",
        "    Conv2 = keras.layers.Conv2D(filters=64,\n",
        "                        kernel_size = 3, \n",
        "                        strides = 1,  # 2\n",
        "                        padding = 'same',                                                \n",
        "                        input_shape = (None,None,64))(Conv1)\n",
        "    Conv2 = keras.layers.LeakyReLU()(Conv2)\n",
        "    # Conv2 = keras.layers.MaxPooling2D((2,2),padding='same')(Conv2)\n",
        "    # Conv2_BN = keras.layers.BatchNormalization()(Conv2)\n",
        "    # Conv2_BN = Dropout(0.5)(Conv2_BN)                        \n",
        "    # PReLu = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(Conv2)\n",
        "    Conv3 = keras.layers.Conv2D(filters=64,\n",
        "                        kernel_size = 1, \n",
        "                        strides = 1,  # 2\n",
        "                        padding = 'same',         \n",
        "                        input_shape = (None,None,64))(Conv2)\n",
        "    # Conv3 = keras.layers.MaxPooling2D((2,2),padding='same')(Conv3)\n",
        "    # Conv3 = keras.layers.LeakyReLU()(Conv3)\n",
        "    \n",
        "    # Concatenate        \n",
        "    x = keras.layers.Add()([Conv3,x])\n",
        "    \n",
        "  # x = keras.layers.MaxPooling2D((2,2),padding='same')(x)\n",
        "  ####Conv 3x3x64x64 + PReLu\n",
        "  x = keras.layers.Conv2D(filters = 64,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.Conv2D(filters = 64,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.Conv2D(filters = 64,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  # 2\n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',\n",
        "                     input_shape = (None,None,64))(x)            \n",
        "  # x = keras.layers.MaxPooling2D((2,2),padding='same')(x)\n",
        "  # x = keras.layers.BatchNormalization()(x)                                                                 \n",
        "  # x = keras.layers.PReLU(alpha_initializer='zeros', alpha_regularizer=None, alpha_constraint=None, shared_axes=[1,2])(x)\n",
        "\n",
        "\n",
        "  ####Conv 3x3x64x48\n",
        "  x = keras.layers.Conv2D(filters = 64,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  \n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',                   \n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.Conv2D(filters = 64,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  \n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',                   \n",
        "                     input_shape = (None,None,64))(x)\n",
        "  x = keras.layers.Conv2D(filters = 48,\n",
        "                     kernel_size = 3, \n",
        "                     strides = 1,  \n",
        "                     padding = 'same',\n",
        "                     activation = 'relu',                   \n",
        "                     input_shape = (None,None,64))(x)\n",
        "  # x = keras.layers.MaxPooling2D((2,2),padding='same')(x)                     \n",
        "  # x = keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  ###########Learning Residual (DCNN)############\n",
        "  \n",
        "\n",
        "  ##Recovery From Subpixel\n",
        "  sub_layer = Lambda(lambda x:tf.nn.depth_to_space(x,4)) \n",
        "  Residual_Output = sub_layer(inputs=x)\n",
        "  \n",
        "\n",
        "  ##Initial Prediction\n",
        "  R = Lambda(lambda x: x[:,:,:,0])(init)\n",
        "  G = Lambda(lambda x: x[:,:,:,1:3])(init)\n",
        "  G = Lambda(lambda x: K.mean(x, axis=3))(G)\n",
        "  B = Lambda(lambda x: x[:,:,:,3])(init)\n",
        "  # print(init.shape)\n",
        "  # print(R.shape)\n",
        "  # print(G.shape)\n",
        "  # print(B.shape)\n",
        "  R = Lambda(lambda x: tf.expand_dims(x, -1))(R)\n",
        "  G = Lambda(lambda x: tf.expand_dims(x, -1))(G)\n",
        "  B = Lambda(lambda x: tf.expand_dims(x, -1))(B)\n",
        "  \n",
        "  #rgb = tf.keras.backend.stack((R, G,B),axis =  3)\n",
        "  # print(R.shape)\n",
        "  rg = keras.layers.Concatenate(axis = 3)([R , G])\n",
        "  rgb = keras.layers.Concatenate(axis = 3)([rg,B])\n",
        "  # print(rgb.shape)\n",
        "  Coarse_Output = keras.layers.UpSampling2D(size=(4, 4), interpolation=\"bilinear\")(rgb) #size=4 , from W/2,H/2 ---> 2W,2H\n",
        "  # Coarse_Output = keras.layers.MaxPooling2D((2,2),padding='same')(Coarse_Output)\n",
        "\n",
        "\n",
        "  ## + \n",
        "  outputs = keras.layers.Add()([Residual_Output,Coarse_Output])\n",
        "  #outputs = Residual_Output\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs, name=\"JDMSR_model\")  \n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"JDMSR_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, None, None, 1 1280        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu (PReLU)                 (None, None, None, 1 128         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, None, None, 1 147584      p_re_lu[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_1 (PReLU)               (None, None, None, 1 128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, None, None, 6 73792       p_re_lu_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_2 (PReLU)               (None, None, None, 6 64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, None, None, 2 0           p_re_lu_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, None, None, 6 147520      lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, None, None, 6 256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, None, None, 6 36928       batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, None, None, 6 256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, None, None, 6 36928       batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, None, None, 6 256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, None, None, 6 4160        batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, None, None, 6 0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, None, None, 6 36928       leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 6 0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, None, None, 6 4160        leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, None, None, 6 0           conv2d_8[0][0]                   \n",
            "                                                                 batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, None, None, 6 4160        add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 6 0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 6 0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, None, None, 6 0           conv2d_11[0][0]                  \n",
            "                                                                 add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, None, None, 6 4160        add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, None, None, 6 0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, None, None, 6 0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, None, None, 6 0           conv2d_14[0][0]                  \n",
            "                                                                 add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, None, None, 6 4160        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, None, None, 6 0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, None, None, 6 0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, None, None, 6 0           conv2d_17[0][0]                  \n",
            "                                                                 add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, None, None, 6 4160        add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, None, None, 6 0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, None, None, 6 0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, None, None, 6 0           conv2d_20[0][0]                  \n",
            "                                                                 add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, None, None, 6 4160        add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, None, None, 6 0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, None, None, 6 0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, None, None, 6 0           conv2d_23[0][0]                  \n",
            "                                                                 add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, None, None, 6 36928       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, None, None, 6 36928       conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, None, None, 2 0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, None, None, 6 36928       conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, None, None)   0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, None, None)   0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, None, None, 6 36928       conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, None, None, 1 0           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, None, None, 1 0           lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, None, None)   0           lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, None, None, 6 36928       conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, None, None, 2 0           lambda_6[0][0]                   \n",
            "                                                                 lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, None, None, 1 0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, None, None, 4 27696       conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, None, None, 3 0           concatenate[0][0]                \n",
            "                                                                 lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, None, None, 3 0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, None, None, 3 0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, None, None, 3 0           lambda_1[0][0]                   \n",
            "                                                                 up_sampling2d[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 928,944\n",
            "Trainable params: 928,560\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V3myfmR3UaX"
      },
      "source": [
        "batch_size = 16\n",
        "lr = 0.001\n",
        "e_num = 50\n",
        "dir_path = 'drive/My Drive/Colab Notebooks/undergraduate_project'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsNBcy5C1Sgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a12dd9b9-9519-4650-e819-2a9caaec6973"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()\n",
        "\n",
        "sgd = SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=1.0)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss = 'mean_squared_error', metrics = ['mse'])\n",
        "\n",
        "checkpoint = ModelCheckpoint(os.path.join(dir_path,'model.hdf5'),verbose=1, monitor='loss', save_best_only=True, save_weights_only=True)\n",
        "rrp = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='min', min_lr=0.000002)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='loss', patience=10, verbose=1, mode='auto')\n",
        "\n",
        "print(train_image.shape)\n",
        "print(train_label.shape)\n",
        "history = model.fit(train_image, train_label, epochs=e_num, batch_size=batch_size,verbose=1,validation_split = 0.1,callbacks=[checkpoint, rrp],shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"JDMSR_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, None, None,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, None, None, 1 1280        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_3 (PReLU)               (None, None, None, 1 128         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, None, None, 1 147584      p_re_lu_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_4 (PReLU)               (None, None, None, 1 128         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, None, None, 6 73792       p_re_lu_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "p_re_lu_5 (PReLU)               (None, None, None, 6 64          conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, None, None, 2 0           p_re_lu_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, None, None, 6 147520      lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, None, None, 6 256         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, None, None, 6 36928       batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, None, None, 6 256         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, None, None, 6 36928       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, None, None, 6 256         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, None, None, 6 4160        batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, None, None, 6 0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, None, None, 6 0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, None, None, 6 0           conv2d_38[0][0]                  \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, None, None, 6 4160        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, None, None, 6 0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, None, None, 6 0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, None, None, 6 0           conv2d_41[0][0]                  \n",
            "                                                                 add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, None, None, 6 4160        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, None, None, 6 0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, None, None, 6 0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, None, None, 6 0           conv2d_44[0][0]                  \n",
            "                                                                 add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, None, None, 6 4160        add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, None, None, 6 0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, None, None, 6 0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, None, None, 6 0           conv2d_47[0][0]                  \n",
            "                                                                 add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, None, None, 6 4160        add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, None, None, 6 0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, None, None, 6 0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, None, None, 6 0           conv2d_50[0][0]                  \n",
            "                                                                 add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, None, None, 6 4160        add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, None, None, 6 0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, None, None, 6 36928       leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_23 (LeakyReLU)      (None, None, None, 6 0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, None, None, 6 4160        leaky_re_lu_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, None, None, 6 0           conv2d_53[0][0]                  \n",
            "                                                                 add_11[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, None, None, 6 36928       add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, None, None, 6 36928       conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_12 (Lambda)              (None, None, None, 2 0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, None, None, 6 36928       conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, None, None)   0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_13 (Lambda)              (None, None, None)   0           lambda_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, None, None, 6 36928       conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_15 (Lambda)              (None, None, None, 1 0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_16 (Lambda)              (None, None, None, 1 0           lambda_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_14 (Lambda)              (None, None, None)   0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, None, None, 6 36928       conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, None, None, 2 0           lambda_15[0][0]                  \n",
            "                                                                 lambda_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_17 (Lambda)              (None, None, None, 1 0           lambda_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, None, None, 4 27696       conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, None, None, 3 0           concatenate_2[0][0]              \n",
            "                                                                 lambda_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, None, None, 3 0           conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, None, None, 3 0           concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, None, None, 3 0           lambda_10[0][0]                  \n",
            "                                                                 up_sampling2d_1[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 928,944\n",
            "Trainable params: 928,560\n",
            "Non-trainable params: 384\n",
            "__________________________________________________________________________________________________\n",
            "(16800, 64, 64, 1)\n",
            "(16800, 128, 128, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "945/945 [==============================] - 102s 56ms/step - loss: 1067.0114 - mse: 1067.0114 - val_loss: 199.6852 - val_mse: 199.6852\n",
            "\n",
            "Epoch 00001: loss improved from inf to 411.73077, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 2/50\n",
            "945/945 [==============================] - 53s 56ms/step - loss: 195.0287 - mse: 195.0287 - val_loss: 163.8533 - val_mse: 163.8533\n",
            "\n",
            "Epoch 00002: loss improved from 411.73077 to 185.95012, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 3/50\n",
            "945/945 [==============================] - 53s 56ms/step - loss: 175.8356 - mse: 175.8356 - val_loss: 147.4516 - val_mse: 147.4516\n",
            "\n",
            "Epoch 00003: loss improved from 185.95012 to 166.49963, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 4/50\n",
            "945/945 [==============================] - 53s 56ms/step - loss: 154.6457 - mse: 154.6457 - val_loss: 137.7859 - val_mse: 137.7859\n",
            "\n",
            "Epoch 00004: loss improved from 166.49963 to 151.86629, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 5/50\n",
            "945/945 [==============================] - 55s 58ms/step - loss: 149.4038 - mse: 149.4038 - val_loss: 136.0666 - val_mse: 136.0666\n",
            "\n",
            "Epoch 00005: loss improved from 151.86629 to 146.22154, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 6/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 143.6524 - mse: 143.6524 - val_loss: 133.3430 - val_mse: 133.3430\n",
            "\n",
            "Epoch 00006: loss improved from 146.22154 to 141.89438, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 7/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 134.2103 - mse: 134.2103 - val_loss: 125.7688 - val_mse: 125.7688\n",
            "\n",
            "Epoch 00007: loss improved from 141.89438 to 136.18105, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 8/50\n",
            "945/945 [==============================] - 54s 58ms/step - loss: 135.7266 - mse: 135.7266 - val_loss: 128.5297 - val_mse: 128.5297\n",
            "\n",
            "Epoch 00008: loss improved from 136.18105 to 132.44745, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 9/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 131.8690 - mse: 131.8690 - val_loss: 121.0637 - val_mse: 121.0637\n",
            "\n",
            "Epoch 00009: loss improved from 132.44745 to 130.59915, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 10/50\n",
            "945/945 [==============================] - 53s 57ms/step - loss: 129.0345 - mse: 129.0345 - val_loss: 120.7235 - val_mse: 120.7235\n",
            "\n",
            "Epoch 00010: loss improved from 130.59915 to 129.16547, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 11/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 126.8485 - mse: 126.8485 - val_loss: 125.5506 - val_mse: 125.5506\n",
            "\n",
            "Epoch 00011: loss improved from 129.16547 to 125.87704, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 12/50\n",
            "945/945 [==============================] - 53s 57ms/step - loss: 123.7305 - mse: 123.7305 - val_loss: 119.3842 - val_mse: 119.3842\n",
            "\n",
            "Epoch 00012: loss improved from 125.87704 to 124.58842, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 13/50\n",
            "945/945 [==============================] - 53s 57ms/step - loss: 122.5637 - mse: 122.5637 - val_loss: 115.0135 - val_mse: 115.0135\n",
            "\n",
            "Epoch 00013: loss improved from 124.58842 to 124.07651, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 14/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 120.1230 - mse: 120.1230 - val_loss: 117.4716 - val_mse: 117.4716\n",
            "\n",
            "Epoch 00014: loss improved from 124.07651 to 120.28497, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 15/50\n",
            "945/945 [==============================] - 53s 57ms/step - loss: 120.7469 - mse: 120.7469 - val_loss: 126.6562 - val_mse: 126.6562\n",
            "\n",
            "Epoch 00015: loss did not improve from 120.28497\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 16/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 113.5502 - mse: 113.5502 - val_loss: 107.3854 - val_mse: 107.3854\n",
            "\n",
            "Epoch 00016: loss improved from 120.28497 to 110.79398, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 17/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 108.5888 - mse: 108.5888 - val_loss: 106.9801 - val_mse: 106.9801\n",
            "\n",
            "Epoch 00017: loss improved from 110.79398 to 108.85506, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 18/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 107.3686 - mse: 107.3686 - val_loss: 106.2072 - val_mse: 106.2072\n",
            "\n",
            "Epoch 00018: loss improved from 108.85506 to 108.26585, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 19/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 108.2487 - mse: 108.2487 - val_loss: 106.6251 - val_mse: 106.6251\n",
            "\n",
            "Epoch 00019: loss improved from 108.26585 to 107.78072, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 20/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 106.2719 - mse: 106.2719 - val_loss: 106.2853 - val_mse: 106.2853\n",
            "\n",
            "Epoch 00020: loss improved from 107.78072 to 107.07700, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "\n",
            "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "Epoch 21/50\n",
            "945/945 [==============================] - 54s 58ms/step - loss: 105.7904 - mse: 105.7904 - val_loss: 104.5058 - val_mse: 104.5058\n",
            "\n",
            "Epoch 00021: loss improved from 107.07700 to 105.78905, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 22/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.9766 - mse: 104.9766 - val_loss: 104.4698 - val_mse: 104.4698\n",
            "\n",
            "Epoch 00022: loss improved from 105.78905 to 105.72776, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 23/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 106.9015 - mse: 106.9015 - val_loss: 104.3601 - val_mse: 104.3601\n",
            "\n",
            "Epoch 00023: loss improved from 105.72776 to 105.60313, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 24/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 106.1765 - mse: 106.1765 - val_loss: 104.2988 - val_mse: 104.2988\n",
            "\n",
            "Epoch 00024: loss improved from 105.60313 to 105.57220, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 25/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.6366 - mse: 105.6366 - val_loss: 104.3422 - val_mse: 104.3422\n",
            "\n",
            "Epoch 00025: loss improved from 105.57220 to 105.46314, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 26/50\n",
            "945/945 [==============================] - 53s 57ms/step - loss: 103.5114 - mse: 103.5114 - val_loss: 104.7263 - val_mse: 104.7263\n",
            "\n",
            "Epoch 00026: loss improved from 105.46314 to 105.36676, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 2e-06.\n",
            "Epoch 27/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.7901 - mse: 104.7901 - val_loss: 104.0797 - val_mse: 104.0797\n",
            "\n",
            "Epoch 00027: loss improved from 105.36676 to 105.30468, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 28/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 103.4493 - mse: 103.4493 - val_loss: 104.1091 - val_mse: 104.1091\n",
            "\n",
            "Epoch 00028: loss improved from 105.30468 to 105.22146, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 29/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.1514 - mse: 105.1514 - val_loss: 104.1079 - val_mse: 104.1079\n",
            "\n",
            "Epoch 00029: loss improved from 105.22146 to 105.17991, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 30/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.3489 - mse: 104.3489 - val_loss: 104.1013 - val_mse: 104.1013\n",
            "\n",
            "Epoch 00030: loss did not improve from 105.17991\n",
            "Epoch 31/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.5851 - mse: 105.5851 - val_loss: 104.0448 - val_mse: 104.0448\n",
            "\n",
            "Epoch 00031: loss did not improve from 105.17991\n",
            "Epoch 32/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.8930 - mse: 105.8930 - val_loss: 104.0660 - val_mse: 104.0660\n",
            "\n",
            "Epoch 00032: loss improved from 105.17991 to 105.15941, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 33/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 103.5495 - mse: 103.5495 - val_loss: 104.0391 - val_mse: 104.0391\n",
            "\n",
            "Epoch 00033: loss did not improve from 105.15941\n",
            "Epoch 34/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.3958 - mse: 105.3958 - val_loss: 104.1384 - val_mse: 104.1384\n",
            "\n",
            "Epoch 00034: loss did not improve from 105.15941\n",
            "Epoch 35/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.9462 - mse: 105.9462 - val_loss: 103.9913 - val_mse: 103.9913\n",
            "\n",
            "Epoch 00035: loss improved from 105.15941 to 105.15643, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 36/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.6962 - mse: 105.6962 - val_loss: 104.0025 - val_mse: 104.0025\n",
            "\n",
            "Epoch 00036: loss improved from 105.15643 to 105.09914, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 37/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.7395 - mse: 104.7395 - val_loss: 104.0070 - val_mse: 104.0070\n",
            "\n",
            "Epoch 00037: loss improved from 105.09914 to 105.08022, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 38/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.9294 - mse: 104.9294 - val_loss: 103.9944 - val_mse: 103.9944\n",
            "\n",
            "Epoch 00038: loss improved from 105.08022 to 105.07992, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 39/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.6406 - mse: 105.6406 - val_loss: 104.0154 - val_mse: 104.0154\n",
            "\n",
            "Epoch 00039: loss did not improve from 105.07992\n",
            "Epoch 40/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.6768 - mse: 104.6768 - val_loss: 103.9793 - val_mse: 103.9793\n",
            "\n",
            "Epoch 00040: loss improved from 105.07992 to 105.06936, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 41/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 103.7662 - mse: 103.7662 - val_loss: 103.9828 - val_mse: 103.9828\n",
            "\n",
            "Epoch 00041: loss improved from 105.06936 to 105.06622, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 42/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.1028 - mse: 104.1028 - val_loss: 103.9397 - val_mse: 103.9397\n",
            "\n",
            "Epoch 00042: loss improved from 105.06622 to 105.01482, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 43/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.5099 - mse: 104.5099 - val_loss: 103.9024 - val_mse: 103.9024\n",
            "\n",
            "Epoch 00043: loss improved from 105.01482 to 104.99219, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 44/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.0300 - mse: 105.0300 - val_loss: 103.9121 - val_mse: 103.9121\n",
            "\n",
            "Epoch 00044: loss did not improve from 104.99219\n",
            "Epoch 45/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.6001 - mse: 105.6001 - val_loss: 103.8821 - val_mse: 103.8821\n",
            "\n",
            "Epoch 00045: loss improved from 104.99219 to 104.98242, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 46/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 104.6676 - mse: 104.6676 - val_loss: 103.8845 - val_mse: 103.8845\n",
            "\n",
            "Epoch 00046: loss improved from 104.98242 to 104.96527, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 47/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 105.3753 - mse: 105.3753 - val_loss: 103.8773 - val_mse: 103.8773\n",
            "\n",
            "Epoch 00047: loss did not improve from 104.96527\n",
            "Epoch 48/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 106.0411 - mse: 106.0411 - val_loss: 103.9102 - val_mse: 103.9102\n",
            "\n",
            "Epoch 00048: loss improved from 104.96527 to 104.94470, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n",
            "Epoch 49/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 103.9400 - mse: 103.9400 - val_loss: 103.8709 - val_mse: 103.8709\n",
            "\n",
            "Epoch 00049: loss did not improve from 104.94470\n",
            "Epoch 50/50\n",
            "945/945 [==============================] - 54s 57ms/step - loss: 103.8736 - mse: 103.8736 - val_loss: 103.8578 - val_mse: 103.8578\n",
            "\n",
            "Epoch 00050: loss improved from 104.94470 to 104.91044, saving model to drive/My Drive/Colab Notebooks/undergraduate_project/model.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TVUNhlptZdZ"
      },
      "source": [
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure()\n",
        "plt.plot()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Accuracy - 2')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='lower right')\n",
        "plt.savefig('model_accuracy2.png')\n",
        "plt.show()\n",
        "\n",
        "plt.plot()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Loss - 2')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.savefig('model_loss2.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSBqGz65q-7S"
      },
      "source": [
        "**Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcbfQPmoq2W1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa7614ac-159a-41b6-e413-9dc300649667"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.layers import Lambda\n",
        "from keras.preprocessing import image\n",
        "import keras\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image \n",
        "from keras import backend as K\n",
        "import os\n",
        "import math\n",
        "import shutil\n",
        "\n",
        "dir_path = 'drive/My Drive/Colab Notebooks/undergraduate_project'\n",
        "model = create_model()\n",
        "model.load_weights(os.path.join(dir_path,'model.hdf5'))\n",
        "\n",
        "input_path = os.path.join(dir_path, 'koda')\n",
        "output_path = os.path.join(dir_path, 'output')\n",
        "output_path2 = os.path.join(dir_path, 'output2')\n",
        "output_path3 = os.path.join(dir_path, 'output3')\n",
        "\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "os.makedirs(output_path)\n",
        "\n",
        "if os.path.exists(output_path2):\n",
        "    shutil.rmtree(output_path2)\n",
        "os.makedirs(output_path2)\n",
        "\n",
        "if os.path.exists(output_path3):\n",
        "    shutil.rmtree(output_path3)\n",
        "os.makedirs(output_path3)\n",
        "\n",
        "entries = os.listdir(input_path)\n",
        "\n",
        "for entry in entries:\n",
        "    # Test Image\n",
        "    print('-----------')\n",
        "    path = input_path+'/'+entry\n",
        "    test_image = Image.open(path)\n",
        "    print(test_image.size)\n",
        "    \n",
        "    if (test_image.size[0]/2)%2 != 0: #odd size\n",
        "      test_image = test_image.resize((test_image.size[0]-1, test_image.size[1]), Image.BICUBIC)\n",
        "    if (test_image.size[1]/2)%2 != 0:\n",
        "      test_image = test_image.resize((test_image.size[0], test_image.size[1]-1), Image.BICUBIC)\n",
        "    path = output_path+'/'+entry\n",
        "    test_image.save(path)\n",
        "    print(test_image.size)\n",
        "\n",
        "    test_image = test_image.resize((test_image.size[0]//2, test_image.size[1]//2), Image.BICUBIC)\n",
        "    print(test_image.size)\n",
        "    test_image_array = np.array(test_image)\n",
        "    test_image_array = bayer_reverse(test_image_array)\n",
        "    test_image_array = test_image_array[:,:,np.newaxis]\n",
        "    test_image = image.array_to_img(test_image_array)\n",
        "    test_image.save(output_path3+'/'+entry)\n",
        "    # print(test_image.shape)\n",
        "    print(test_image.size)\n",
        "    \n",
        "    \n",
        "    # print(test_image.shape)\n",
        "    test_image = np.array(test_image)\n",
        "    test_image = test_image[np.newaxis,:,:]\n",
        "    try:\n",
        "      out = model.predict(test_image)\n",
        "      out = out[0]\n",
        "      out = image.array_to_img(out)\n",
        "      path = output_path2+'/'+entry\n",
        "      out.save(path)\n",
        "\n",
        "    except:\n",
        "      path = input_path+'/'+entry\n",
        "      test_image = Image.open(path)\n",
        "      if (test_image.size[0]//scale)%2 != 0:\n",
        "        test_image = test_image.resize(((test_image.size[0]//scale) - 1, test_image.size[1]//scale), Image.BICUBIC)\n",
        "      if (test_image.size[1]//scale)%2 != 0:\n",
        "        test_image = test_image.resize((test_image.size[0]//scale , (test_image.size[1]//scale) - 1), Image.BICUBIC)\n",
        "      test_image_array = np.array(test_image)\n",
        "      test_image_array = bayer_reverse(test_image_array)\n",
        "      test_image_array = test_image_array[:,:,np.newaxis]\n",
        "      test_image = image.array_to_img(test_image_array)\n",
        "      print(test_image.size)\n",
        "      test_image.save(output_path3+'/'+entry)\n",
        "      test_image = np.array(test_image)\n",
        "      test_image = test_image[np.newaxis,:,:]\n",
        "      out = model.predict(test_image)\n",
        "      out = out[0]\n",
        "      out = image.array_to_img(out)\n",
        "      path = output_path2+'/'+entry\n",
        "      out.save(path)\n",
        "\n",
        "\n",
        "    # print(out.shape)\n",
        "    \n",
        "\n",
        "# print(len(entries))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(321, 481)\n",
            "(320, 480)\n",
            "(160, 240)\n",
            "(160, 240)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n",
            "-----------\n",
            "(481, 321)\n",
            "(480, 320)\n",
            "(240, 160)\n",
            "(240, 160)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY5gyote19PB"
      },
      "source": [
        "**Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzVpCLEFoX8T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60146fba-ccbb-44dd-caaa-6b9e30a32c21"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "from google.colab.patches import cv2_imshow\n",
        "from skimage.measure import compare_ssim\n",
        "from skimage.measure import compare_ssim as ssim\n",
        "\n",
        "def calculate_psnr(original, contrast):\n",
        "    mse = np.mean((original - contrast) ** 2)\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    max_value = 255.0\n",
        "    return 10 * math.log10(max_value**2 / mse)    \n",
        "    \n",
        "\n",
        "path = 'drive/My Drive/Colab Notebooks/undergraduate_project'\n",
        "input_path = os.path.join(path,'output')\n",
        "output_path = os.path.join(path,'output2')\n",
        "entries = os.listdir(input_path)\n",
        "count = 0\n",
        "total_psnr = 0.\n",
        "total_ssim = 0.\n",
        "\n",
        "for entry in entries:  \n",
        "  img1 = cv2.imread(os.path.join(input_path,entry))\n",
        "  img2 = cv2.imread(os.path.join(output_path,entry))\n",
        "  img1 = cv2.resize(img1, (img2.shape[1], img2.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "    \n",
        "\n",
        "\n",
        "  # PSNR\n",
        "  psnr = calculate_psnr(img1,img2)\n",
        "  print(\"PSNR-{0}: {1:.10f}dB\".format(entry,psnr))\n",
        "\n",
        "  # SSIM\n",
        "  ssim = compare_ssim(img1, img2, data_range=255.0 - 0.0,multichannel=True)\n",
        "  print(\"SSIM-{0}: {1:.10f}\".format(entry,ssim))\n",
        "\n",
        "  total_psnr += psnr\n",
        "  total_ssim += ssim\n",
        "  count += 1\n",
        "print(count)\n",
        "\n",
        "total_psnr = total_psnr / count\n",
        "total_ssim = total_ssim / count\n",
        "print(\"\\n=====================================\")\n",
        "print(\"Average PSNR:{:.10f}\".format(total_psnr))\n",
        "print(\"Average SSIM:{:.10f}\".format(total_ssim))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR-126007.jpg: 27.5680608067dB\n",
            "SSIM-126007.jpg: 0.8073178361\n",
            "PSNR-24077.jpg: 27.8912181256dB\n",
            "SSIM-24077.jpg: 0.8526484924\n",
            "PSNR-157055.jpg: 27.3065674168dB\n",
            "SSIM-157055.jpg: 0.8073012958\n",
            "PSNR-103070.jpg: 27.2596979936dB\n",
            "SSIM-103070.jpg: 0.8832143149\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: DEPRECATED: skimage.measure.compare_ssim has been moved to skimage.metrics.structural_similarity. It will be removed from skimage.measure in version 0.18.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "PSNR-156065.jpg: 30.3648811152dB\n",
            "SSIM-156065.jpg: 0.7835894835\n",
            "PSNR-170057.jpg: 29.1322365640dB\n",
            "SSIM-170057.jpg: 0.9146643532\n",
            "PSNR-76053.jpg: 27.5901472968dB\n",
            "SSIM-76053.jpg: 0.8072015272\n",
            "PSNR-78004.jpg: 28.7543717853dB\n",
            "SSIM-78004.jpg: 0.8607183036\n",
            "PSNR-38092.jpg: 28.6167747435dB\n",
            "SSIM-38092.jpg: 0.8504220508\n",
            "PSNR-253027.jpg: 27.2006672528dB\n",
            "SSIM-253027.jpg: 0.7209558015\n",
            "PSNR-16077.jpg: 27.3801107935dB\n",
            "SSIM-16077.jpg: 0.7811798712\n",
            "PSNR-163085.jpg: 30.6100132108dB\n",
            "SSIM-163085.jpg: 0.9133935782\n",
            "PSNR-219090.jpg: 28.7127667467dB\n",
            "SSIM-219090.jpg: 0.8668010378\n",
            "PSNR-58060.jpg: 29.7846089887dB\n",
            "SSIM-58060.jpg: 0.7441327902\n",
            "PSNR-296007.jpg: 29.1679615768dB\n",
            "SSIM-296007.jpg: 0.8839202206\n",
            "PSNR-101085.jpg: 29.0546724208dB\n",
            "SSIM-101085.jpg: 0.7822121406\n",
            "PSNR-119082.jpg: 28.7310037200dB\n",
            "SSIM-119082.jpg: 0.8489635208\n",
            "PSNR-300091.jpg: 27.9209532266dB\n",
            "SSIM-300091.jpg: 0.8576244869\n",
            "PSNR-167062.jpg: 31.9829932324dB\n",
            "SSIM-167062.jpg: 0.9022111714\n",
            "PSNR-45096.jpg: 35.9661096598dB\n",
            "SSIM-45096.jpg: 0.9433544105\n",
            "PSNR-260058.jpg: 27.4668573298dB\n",
            "SSIM-260058.jpg: 0.8866054570\n",
            "PSNR-89072.jpg: 28.5409422621dB\n",
            "SSIM-89072.jpg: 0.8348841971\n",
            "PSNR-175032.jpg: 29.1906802938dB\n",
            "SSIM-175032.jpg: 0.7758794649\n",
            "PSNR-86016.jpg: 28.1676152175dB\n",
            "SSIM-86016.jpg: 0.6665816224\n",
            "PSNR-42049.jpg: 28.7231664277dB\n",
            "SSIM-42049.jpg: 0.9543444788\n",
            "PSNR-295087.jpg: 29.0173586920dB\n",
            "SSIM-295087.jpg: 0.8282153312\n",
            "PSNR-86000.jpg: 27.5728327614dB\n",
            "SSIM-86000.jpg: 0.8448064031\n",
            "PSNR-220075.jpg: 28.5792062590dB\n",
            "SSIM-220075.jpg: 0.8989218430\n",
            "PSNR-41033.jpg: 31.5379969580dB\n",
            "SSIM-41033.jpg: 0.8939059994\n",
            "PSNR-304034.jpg: 28.4028940382dB\n",
            "SSIM-304034.jpg: 0.8326857362\n",
            "PSNR-291000.jpg: 27.5781873507dB\n",
            "SSIM-291000.jpg: 0.6279554271\n",
            "PSNR-43074.jpg: 34.7547210507dB\n",
            "SSIM-43074.jpg: 0.9155768318\n",
            "PSNR-8023.jpg: 27.5437388285dB\n",
            "SSIM-8023.jpg: 0.9123314557\n",
            "PSNR-253055.jpg: 28.0335120928dB\n",
            "SSIM-253055.jpg: 0.8784299331\n",
            "PSNR-216081.jpg: 29.2585623525dB\n",
            "SSIM-216081.jpg: 0.8157456694\n",
            "PSNR-227092.jpg: 28.1465836167dB\n",
            "SSIM-227092.jpg: 0.8956630009\n",
            "PSNR-189080.jpg: 28.4904199849dB\n",
            "SSIM-189080.jpg: 0.9332148234\n",
            "PSNR-229036.jpg: 27.7749621382dB\n",
            "SSIM-229036.jpg: 0.7664408596\n",
            "PSNR-208001.jpg: 27.7692819889dB\n",
            "SSIM-208001.jpg: 0.8306484461\n",
            "PSNR-241004.jpg: 29.1591628560dB\n",
            "SSIM-241004.jpg: 0.9072129287\n",
            "PSNR-109053.jpg: 27.2254374956dB\n",
            "SSIM-109053.jpg: 0.7691196806\n",
            "PSNR-69040.jpg: 30.2109548087dB\n",
            "SSIM-69040.jpg: 0.8751979944\n",
            "PSNR-97033.jpg: 28.1936049367dB\n",
            "SSIM-97033.jpg: 0.8603185121\n",
            "PSNR-69015.jpg: 31.7367005482dB\n",
            "SSIM-69015.jpg: 0.9065720389\n",
            "PSNR-19021.jpg: 27.7092223489dB\n",
            "SSIM-19021.jpg: 0.7787501402\n",
            "PSNR-182053.jpg: 27.7152875137dB\n",
            "SSIM-182053.jpg: 0.8829075631\n",
            "PSNR-14037.jpg: 32.9943758123dB\n",
            "SSIM-14037.jpg: 0.9352744931\n",
            "PSNR-54082.jpg: 28.3945647501dB\n",
            "SSIM-54082.jpg: 0.8523021437\n",
            "PSNR-223061.jpg: 28.4180410936dB\n",
            "SSIM-223061.jpg: 0.8692800887\n",
            "PSNR-86068.jpg: 27.5633451175dB\n",
            "SSIM-86068.jpg: 0.7374345885\n",
            "PSNR-12084.jpg: 27.6481528707dB\n",
            "SSIM-12084.jpg: 0.8047834912\n",
            "PSNR-134035.jpg: 27.9506964737dB\n",
            "SSIM-134035.jpg: 0.8956376459\n",
            "PSNR-271035.jpg: 27.4618366825dB\n",
            "SSIM-271035.jpg: 0.8258260767\n",
            "PSNR-145086.jpg: 28.1287920697dB\n",
            "SSIM-145086.jpg: 0.7675572392\n",
            "PSNR-3096.jpg: 27.4072532689dB\n",
            "SSIM-3096.jpg: 0.9035139743\n",
            "PSNR-147091.jpg: 28.4547645901dB\n",
            "SSIM-147091.jpg: 0.8449970618\n",
            "PSNR-167083.jpg: 28.4410757913dB\n",
            "SSIM-167083.jpg: 0.7503277967\n",
            "PSNR-148089.jpg: 28.7900745020dB\n",
            "SSIM-148089.jpg: 0.8049803188\n",
            "PSNR-210088.jpg: 27.5738974640dB\n",
            "SSIM-210088.jpg: 0.8273109296\n",
            "PSNR-62096.jpg: 28.8350191933dB\n",
            "SSIM-62096.jpg: 0.8680921113\n",
            "PSNR-130026.jpg: 27.4820401488dB\n",
            "SSIM-130026.jpg: 0.7081625689\n",
            "PSNR-285079.jpg: 27.5138355221dB\n",
            "SSIM-285079.jpg: 0.7778202706\n",
            "PSNR-108082.jpg: 27.5021629566dB\n",
            "SSIM-108082.jpg: 0.7134331337\n",
            "PSNR-160068.jpg: 28.5677182175dB\n",
            "SSIM-160068.jpg: 0.9132186809\n",
            "PSNR-148026.jpg: 28.8584366999dB\n",
            "SSIM-148026.jpg: 0.8271564819\n",
            "PSNR-41069.jpg: 28.1938788031dB\n",
            "SSIM-41069.jpg: 0.7778208250\n",
            "PSNR-101087.jpg: 28.9714013309dB\n",
            "SSIM-101087.jpg: 0.8939030423\n",
            "PSNR-55073.jpg: 28.8971281404dB\n",
            "SSIM-55073.jpg: 0.7454638766\n",
            "PSNR-196073.jpg: 28.1876746704dB\n",
            "SSIM-196073.jpg: 0.7632516459\n",
            "PSNR-108005.jpg: 27.5166383166dB\n",
            "SSIM-108005.jpg: 0.8047144860\n",
            "PSNR-361010.jpg: 28.0011356916dB\n",
            "SSIM-361010.jpg: 0.8227433671\n",
            "PSNR-385039.jpg: 28.3867242321dB\n",
            "SSIM-385039.jpg: 0.8244674361\n",
            "PSNR-159008.jpg: 29.3897428809dB\n",
            "SSIM-159008.jpg: 0.8850947037\n",
            "PSNR-302008.jpg: 35.0518425534dB\n",
            "SSIM-302008.jpg: 0.9597578441\n",
            "PSNR-85048.jpg: 27.4129903480dB\n",
            "SSIM-85048.jpg: 0.8202499982\n",
            "PSNR-304074.jpg: 27.8944971632dB\n",
            "SSIM-304074.jpg: 0.8215055446\n",
            "PSNR-351093.jpg: 28.5740175364dB\n",
            "SSIM-351093.jpg: 0.8019636186\n",
            "PSNR-106024.jpg: 27.5667565870dB\n",
            "SSIM-106024.jpg: 0.9104977295\n",
            "PSNR-197017.jpg: 28.4682486285dB\n",
            "SSIM-197017.jpg: 0.8428370388\n",
            "PSNR-65033.jpg: 28.1504365981dB\n",
            "SSIM-65033.jpg: 0.7562366258\n",
            "PSNR-102061.jpg: 27.5071505405dB\n",
            "SSIM-102061.jpg: 0.8390645738\n",
            "PSNR-143090.jpg: 32.1231377306dB\n",
            "SSIM-143090.jpg: 0.9288290333\n",
            "PSNR-299086.jpg: 32.7759108429dB\n",
            "SSIM-299086.jpg: 0.8995196010\n",
            "PSNR-37073.jpg: 28.1562997982dB\n",
            "SSIM-37073.jpg: 0.8742380499\n",
            "PSNR-306005.jpg: 29.7954226148dB\n",
            "SSIM-306005.jpg: 0.8582558609\n",
            "PSNR-66053.jpg: 32.3633345897dB\n",
            "SSIM-66053.jpg: 0.8593531107\n",
            "PSNR-21077.jpg: 27.2105648386dB\n",
            "SSIM-21077.jpg: 0.7886220893\n",
            "PSNR-108070.jpg: 27.7009610904dB\n",
            "SSIM-108070.jpg: 0.7251694055\n",
            "PSNR-376043.jpg: 28.3221842563dB\n",
            "SSIM-376043.jpg: 0.8224547724\n",
            "PSNR-296059.jpg: 27.4583635574dB\n",
            "SSIM-296059.jpg: 0.8593679895\n",
            "PSNR-33039.jpg: 28.5834360339dB\n",
            "SSIM-33039.jpg: 0.8429781371\n",
            "PSNR-175043.jpg: 27.9140669118dB\n",
            "SSIM-175043.jpg: 0.8611990527\n",
            "PSNR-236037.jpg: 27.5385971505dB\n",
            "SSIM-236037.jpg: 0.7846687985\n",
            "PSNR-42012.jpg: 30.1987557168dB\n",
            "SSIM-42012.jpg: 0.9184754770\n",
            "PSNR-87046.jpg: 27.7131797851dB\n",
            "SSIM-87046.jpg: 0.8913953492\n",
            "PSNR-38082.jpg: 29.2914994042dB\n",
            "SSIM-38082.jpg: 0.8330878297\n",
            "PSNR-123074.jpg: 27.8215108944dB\n",
            "SSIM-123074.jpg: 0.9142658650\n",
            "PSNR-241048.jpg: 27.5842258656dB\n",
            "SSIM-241048.jpg: 0.8072427402\n",
            "PSNR-105025.jpg: 27.7295169029dB\n",
            "SSIM-105025.jpg: 0.8530537964\n",
            "PSNR-69020.jpg: 30.8241018113dB\n",
            "SSIM-69020.jpg: 0.8026112898\n",
            "100\n",
            "\n",
            "=====================================\n",
            "Average PSNR:28.7675712387\n",
            "Average SSIM:0.8380621223\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}